{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mushfiq-hussain/info5731_assignment2/blob/main/hussain_mushfiq_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Research Question:\n",
        "How does daily weather conditions affect mood and productivity in office workers?\n",
        "\n",
        "Data Needed:\n",
        "Weather Data: Daily weather variables such as temperature, humidity, precipitation, wind speed, and cloud cover. This data can be collected from local meteorological stations or weather APIs.\n",
        "Mood and Productivity Data: Daily self-reported mood and productivity ratings from office workers. This data can be collected through surveys or mobile applications.\n",
        "Amount of Data Needed:\n",
        "Weather Data: At least one year of daily weather data to capture seasonal variations.\n",
        "Mood and Productivity Data: Daily data for the same time period as the weather data, collected from a representative sample of office workers.\n",
        "Steps for Data Collection and Saving:\n",
        "Weather Data Collection:\n",
        "Identify local meteorological stations or weather APIs that provide historical daily weather data.\n",
        "Retrieve daily weather data for the desired location, including temperature, humidity, precipitation, wind speed, and cloud cover.\n",
        "Save the weather data in a structured format such as CSV or JSON, with each row representing a single day and columns for each weather variable.\n",
        "Mood and Productivity Data Collection:\n",
        "Develop a survey or mobile application to collect daily mood and productivity ratings from office workers.\n",
        "Ensure that the survey or application is user-friendly and can easily capture data without causing disruptions to the participants' workflow.\n",
        "Recruit a representative sample of office workers to participate in the study.\n",
        "Collect daily mood and productivity ratings from participants over the same time period as the weather data.\n",
        "Save the mood and productivity data in a structured format such as CSV or database tables, with each row representing a single day and columns for mood rating, productivity rating, and participant ID.\n",
        "Data Storage and Management:\n",
        "Create a dedicated storage system for the collected data, ensuring it is secure and accessible to researchers.\n",
        "Organize the weather and mood/productivity data into separate datasets or tables.\n",
        "Implement regular backups to prevent data loss.\n",
        "Document the data collection process thoroughly, including any preprocessing steps or data cleaning procedures.\n",
        "\n",
        "By following these steps, one can collect and save the necessary data to analyze the relationship between daily weather variability and mood/productivity in office workers.\n",
        "\n",
        "''''''"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import csv\n",
        "\n",
        "# Generate simulated weather data\n",
        "def generate_weather_data():\n",
        "    weather_data = []\n",
        "    for _ in range(1000):\n",
        "        temperature = round(random.uniform(10, 30), 2)  # Temperature in Celsius\n",
        "        humidity = round(random.uniform(30, 80), 2)  # Humidity in percentage\n",
        "        precipitation = round(random.uniform(0, 10), 2)  # Precipitation in mm\n",
        "        wind_speed = round(random.uniform(0, 20), 2)  # Wind speed in km/h\n",
        "        cloud_cover = round(random.uniform(0, 100), 2)  # Cloud cover in percentage\n",
        "        weather_data.append([temperature, humidity, precipitation, wind_speed, cloud_cover])\n",
        "    return weather_data\n",
        "\n",
        "# Generate simulated mood and productivity data\n",
        "def generate_mood_productivity_data():\n",
        "    mood_productivity_data = []\n",
        "    for _ in range(1000):\n",
        "        mood_rating = random.randint(1, 10)  # Mood rating on a scale of 1 to 10\n",
        "        productivity_rating = random.randint(1, 10)  # Productivity rating on a scale of 1 to 10\n",
        "        mood_productivity_data.append([mood_rating, productivity_rating])\n",
        "    return mood_productivity_data\n",
        "\n",
        "# Combine weather and mood/productivity data into a single dataset\n",
        "def combine_data(weather_data, mood_productivity_data):\n",
        "    combined_data = []\n",
        "    for i in range(1000):\n",
        "        combined_data.append(weather_data[i] + mood_productivity_data[i])\n",
        "    return combined_data\n",
        "\n",
        "# Save the combined dataset to a CSV file\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Temperature (C)', 'Humidity (%)', 'Precipitation (mm)', 'Wind Speed (km/h)', 'Cloud Cover (%)', 'Mood Rating', 'Productivity Rating'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Main function to generate and save the dataset\n",
        "def main():\n",
        "    weather_data = generate_weather_data()\n",
        "    mood_productivity_data = generate_mood_productivity_data()\n",
        "    combined_data = combine_data(weather_data, mood_productivity_data)\n",
        "    save_to_csv(combined_data, 'office_worker_data.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a03cde4-713a-4e07-c4d2-90eca1ba21a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error retrieving data from ACM Digital Library\n",
            "No articles found.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "def get_acm_articles(keyword, num_articles=1000):\n",
        "    base_url = \"https://dl.acm.org/doi/abs/\"\n",
        "    api_url = \"https://dl.acm.org/doi/search\"\n",
        "    headers = {\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "    params = {\n",
        "        \"startYear\": \"2014\",\n",
        "        \"endYear\": \"2024\",\n",
        "        \"q\": f\"all:{keyword}\",\n",
        "        \"pageSize\": num_articles\n",
        "    }\n",
        "    response = requests.get(api_url, params=params, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        articles = data.get(\"items\", [])\n",
        "        return articles\n",
        "    else:\n",
        "        print(\"Error retrieving data from ACM Digital Library\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(articles, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Title', 'Venue', 'Year', 'Authors', 'Abstract'])\n",
        "        for article in articles:\n",
        "            title = article.get('title', 'N/A')\n",
        "            venue = article.get('venue', 'N/A')\n",
        "            year = article.get('year', 'N/A')\n",
        "            authors = ', '.join(article.get('authors', ['N/A']))\n",
        "            abstract = article.get('abstract', 'N/A')\n",
        "            writer.writerow([title, venue, year, authors, abstract])\n",
        "\n",
        "def main():\n",
        "    keyword = \"XYZ\"\n",
        "    num_articles = 1000\n",
        "    articles = get_acm_articles(keyword, num_articles)\n",
        "    if articles:\n",
        "        filename = f\"{keyword}_articles.csv\"\n",
        "        save_to_csv(articles, filename)\n",
        "        print(f\"{num_articles} articles with keyword '{keyword}' saved to {filename}\")\n",
        "    else:\n",
        "        print(\"No articles found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import csv\n",
        "\n",
        "# Twitter API credentials\n",
        "consumer_key = \"your_consumer_key\"\n",
        "consumer_secret = \"your_consumer_secret\"\n",
        "access_token = \"your_access_token\"\n",
        "access_token_secret = \"your_access_token_secret\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "def get_tweets_by_hashtag(hashtag, num_tweets=1000):\n",
        "    tweets = []\n",
        "    for tweet in tweepy.Cursor(api.search, q=f\"#{hashtag}\", tweet_mode='extended').items(num_tweets):\n",
        "        tweets.append([tweet.user.screen_name, tweet.created_at, tweet.full_text, tweet.retweet_count, tweet.favorite_count])\n",
        "    return tweets\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Username', 'Date', 'Text', 'Retweets', 'Favorites'])\n",
        "        writer.writerows(data)\n",
        "\n",
        "def main():\n",
        "    hashtag = \"your_hashtag\"\n",
        "    num_tweets = 1000\n",
        "    tweets = get_tweets_by_hashtag(hashtag, num_tweets)\n",
        "    if tweets:\n",
        "        filename = f\"{hashtag}_tweets.csv\"\n",
        "        save_to_csv(tweets, filename)\n",
        "        print(f\"{num_tweets} tweets with hashtag '{hashtag}' saved to {filename}\")\n",
        "    else:\n",
        "        print(\"No tweets found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All in all, completing web scraping assignments was a rewarding educational experience. Comprehending the fundamental ideas of web scraping,\n",
        "such as CSS selectors, APIs, and HTML structure, proved to be essential for efficiently obtaining data from many online sources.\n",
        "Understanding the fundamentals of pagination, handling dynamic information, and identifying and navigating through HTML components was very\n",
        "helpful when learning how to extract data from internet sources.\n",
        "While working on web scraping tasks, I encountered specific difficulties when collecting data from certain websites. Some websites have complex\n",
        "HTML structures or dynamic content loaded via JavaScript, making it challenging to extract the desired data using traditional scraping methods.\n",
        "The ability to gather and analyze data from online sources is highly relevant across various fields of study, including mine\n",
        "In general, there are a lot of chances for study, analysis, and decision-making across a variety of fields when one can collect and use data from\n",
        "internet sources.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}